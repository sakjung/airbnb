---
title: "part C"
output: html_document
---

```{r}
rm(list=ls())
library(data.table)
library(tidytext)
library(RSQLite)
library(foreach)
library(doParallel)
library(dplyr)
library(udpipe)
library(stm)
library(anacor)
library(stargazer)
library(pscl)
```

```{r }
# db has been made in part A 
con <- dbConnect(SQLite(), "south_aegean.sqlite")

dbListTables(con)

all_data <- dbGetQuery(con, "SELECT * FROM all_data")
dbDisconnect(con)
# For this part we follow the work we did on lab 8 
# in essence we want to build a topic model 
# where the topic is dependent on the review score 
# and the price

data_for_stm <- all_data %>% 
  select(review_id,comments, review_scores_rating,listing_id,price) %>% 
  mutate(price = as.numeric(gsub("\\$","",price))) %>%
  rename(text = comments) %>% 
  na.omit()

# remove same reviews
data_for_stm <- data_for_stm[!duplicated(data_for_stm$review_id),]

rm(all_data)
gc()
```

```{r}
# Tokenize before 
# feeding them in udpipe 
# saves a lot of time during processing
data("stop_words")

data_for_stm %>% select(review_id,text) %>% 
  unnest_tokens(word,text) %>% 
  anti_join(stop_words) %>% 
  count(word,review_id) -> reviews_tokenized

# Remove those less than 3 chars 
reviews_tokenized$word_length <- nchar(reviews_tokenized$word)

reviews_tokenized %>% 
  filter(word_length>3 & word_length<15) -> reviews_tokenized

cl <- makePSOCKcluster(detectCores()-1)
registerDoParallel(cl)

# subset original vector of words to exclude words with non-ASCII char
only_en <- parSapply(cl = cl, reviews_tokenized$word, function(row) iconv(row, "latin1", "ASCII", sub=""))
reviews_tokenized$word <- only_en

stopCluster(cl)
rm(only_en)
gc()

# find out a word with repetitive characters or patterns of characters
# e.g. aaaa or wowwowwowwow
customstopwords <- reviews_tokenized$word[grepl("\\b(\\S+?)\\1\\1\\S*\\b", reviews_tokenized$word)]

# remove useless words
reviews_tokenized <- reviews_tokenized %>% 
  filter(word != "") %>%
  filter(!word %in% customstopwords)

# Do some tf-idf weighting 
# so we remove extremely rare and 
# extremely common parts 
reviews_tokenized <- reviews_tokenized %>% 
  bind_tf_idf(word,review_id,n) %>% 
  filter(between(tf_idf,0.1,0.8)) 

language <- udpipe_download_model(language="english",overwrite = F)
ud_model <- udpipe_load_model(language$file_model)

rm(language)
gc()
```

```{r}
# do udpipe on tokenised dataframe, otherwise very slow

cl <- makePSOCKcluster(detectCores()-1)
registerDoParallel(cl)

clusterEvalQ(cl, {
  library(dplyr)
  library(udpipe)
  library(data.table)
  # some more library for pre processing
})

split_size <- 50000
total_number_of_chunks <- ceiling(nrow(reviews_tokenized)/split_size)

annotation_list <- foreach(i=1:total_number_of_chunks) %dopar% {
  
  if (i == 1){
    this_start_row <- i 
    this_last_row <- i*split_size
  } else {
    this_start_row <- (i-1)*split_size+1
    this_last_row <- i*split_size
  }
  
  annotated_review <- as.data.table(udpipe_annotate(ud_model, 
                                     x = reviews_tokenized$word[this_start_row:this_last_row],
                                     doc_id = reviews_tokenized$review_id[this_start_row:this_last_row]))

  return(annotated_review)
}

stopCluster(cl)

annotated_reviews <- rbindlist(annotation_list)
rm(annotation_list)
gc()

con <- dbConnect(SQLite(), "south_aegean.sqlite")
dbWriteTable(con, "annotated_reviews", annotated_reviews, overwrite=TRUE)
dbListTables(con)
# when laoding annotated_reviews from db
# annotated_reviews <- dbGetQuery(con, "SELECT * FROM annotated_reviews")
dbDisconnect(con)
```

```{r}
annotated_reviews <- annotated_reviews %>%
  filter(upos %in% c("NOUN","ADJ","ADV")) %>%
  select(doc_id,lemma) %>%
  rename(review_id = doc_id)

annotated_reviews <- annotated_reviews %>% group_by(review_id) %>%
  summarise(annotated_comments = paste(lemma,collapse = " "))

data_for_stm$review_id <- as.character(data_for_stm$review_id)

data_for_stm <- annotated_reviews %>% 
  left_join(data_for_stm)

# back up
con <- dbConnect(SQLite(), "south_aegean.sqlite")
dbWriteTable(con, "data_for_stm", data_for_stm, overwrite=TRUE)
dbListTables(con)
dbDisconnect(con)

# finished preparing data for LDA
```


```{r}
processed_data <- textProcessor(data_for_stm$annotated_comments,
                                metadata = data_for_stm,
                                customstopwords = c("airbnb","south_aegean"),
                                stem = F)

#keep only the vocabulary that appears at the 
#1% of all documents
threshold <- round(1/100 * length(processed_data$documents),0)

out <- prepDocuments(processed_data$documents,
                     processed_data$vocab,
                     processed_data$meta,
                     lower.thresh = threshold)

# Check the out$vocab vector
# You will see that there are some 
# terms that should not be there because of 
# misspelings such as nicly and nicely

# this makes error in stm!
# vocab <- out$vocab
# vocab_prob <- c("accommodat", "didn", "lovly")
# out$vocab <- vocab[! vocab %in% vocab_prob]

# Execute an stm model 
# here using the following prevalence function 
# prevalence =~ review_scores_rating + price
# with K=10 and the topic-word probabilities 
# to depend on price and review score rating

# Notice that you need to select the optimal number of topics 
# this can be done with the searchK function
# see lab 8 

# Papa Nikos Big Gift: If you set K to zero the stm
# algorithm is going to give you the right number of topics. 
# For this to work you need to set the init.type to spectral
```

```{r stm}
numtopics <- searchK(out$documents,out$vocab,K=seq(from=4, to=10,by=1))
plot(numtopics)
# back up
save(numtopics, file = "numtopics.rda")
# search K result shows 6 is the proper number out of 10 number of K
# which can be an alternative way 
# when smaller size of topics are reqired (K)


# The held-out likelihood is highest between 60 and 80, 
# and the residuals are lowest around 60, 
# so perhaps a good number of topics would be around there.
# Semantic coherence is maximized when the most probable words
# in a given topic frequently co-occur together, and it’s a metric
# that correlates well with human judgment of topic quality. 
# Having high semantic coherence is relatively easy, though, 
# if you only have a few topics dominated by very common words, 
# so you want to look at both semantic coherence and exclusivity of words to topics.
# It’s a tradeoff. Read more about semantic coherence in the original paper about it.

airbnbfit <- stm(documents = out$documents,
                 vocab = out$vocab,
                 K = 0,
                 prevalence =~ price+review_scores_rating,
                 max.em.its = 75, 
                 data = out$meta,
                 reportevery=3,
                 # gamma.prior = "L1",
                 sigma.prior = 0.7,
                 init.type = "Spectral")

# airbnbfit_6 <- stm(documents = out$documents,
#                    vocab = out$vocab,
#                    K = 6,
#                    prevalence =~ price+review_scores_rating,
#                    max.em.its = 75, 
#                    data = out$meta,
#                    reportevery=3,
#                    # gamma.prior = "L1",
#                    sigma.prior = 0.7,
#                    init.type = "Spectral")  

# Optimal number of topics = 47 if K = 0

topicQuality(airbnbfit, documents = out$documents)
plot(airbnbfit,labeltype = "prob")

```

```{r}

topic_summary <- summary(airbnbfit)
topic_proportions <- colMeans(airbnbfit$theta)
# The number here is the number of topics 
# don't forget to change it for an optimal 
# solution

topic_labels <- paste0("topic_",1:47)
table_towrite_labels <- data.frame()

for(i in 1:length(topic_summary$topicnums)){
  
  row_here <- tibble(topicnum= topic_summary$topicnums[i],
                     topic_label = topic_labels[i],
                     proportion = 100*round(topic_proportions[i],4),
                     frex_words = paste(topic_summary$frex[i,1:7],
                                        collapse = ", "))
  table_towrite_labels <- rbind(row_here,table_towrite_labels)
}

table_towrite_labels %>% arrange(topicnum)

# For a complete solution you need to label 
# your topics. Use the cloud() function to get 
# the dominant word

for (i in 1:length(topic_summary$topicnums)) {
  cloud(airbnbfit,topic = i, max.words = 15)
}

# By analysing the word clouds
# topic can be labelled as follows
topic_labels <- c("window", "short walking distance", "view (positive)", "boat activity", 
  "transport (taxi)", "tourist plan", "tip", "market in Thira", "taverna in village", "about studio",
  "shop nearby (positive)", "breakfast by host", "vacation", "shower issue", "Mykonos and Rhodes island",
  "close restaurant", "question on accommodation", "pool", "ferry ride", "private property",
  "Naxos and Paros island", "morning and dinner quality", "Milos island", "bathroom condition",
  "about house (positive)", "breakfast", "late (evening or night) flight", "about flat (positive)",
  "Fira", "airport", "distance", "Caldera (crowd at the spot)", "Service quality", "staff", 
  "city/town (positive)", "center", "Maria's cave (positive)", "busy and noisy spot", 
  "eating and drinking", "spacious room", "host (praise)", "beach (positive)", "luggage",
  "apartment (positive)", "quick response (positive)", "highly satisfied", "accessibility")

table_towrite_labels$topic_label <- topic_labels

# back up
con <- dbConnect(SQLite(), "south_aegean.sqlite")
dbWriteTable(con, "table_towrite_labels", table_towrite_labels, overwrite=TRUE)
dbListTables(con)
dbDisconnect(con)
rm(con)
```

```{r}
# Estimate the effects of price and review score 
# on topic probability

effects <- estimateEffect(~review_scores_rating+price,
                          stmobj = airbnbfit,
                          metadata = out$meta )

# Effect of review score on topic 
# probability 
plot(effects, covariate = "review_scores_rating",
     topics = c(1:47),
     model = airbnbfit, method = "difference",
     cov.value1 = "100", cov.value2 = "0",
     xlab = "Low Rating ... High Rating",
     xlim = c(-0.01,0.01),
     main = "",
     custom.labels =topic_labels,
     labeltype = "custom")

# topics which seem to have significant predictive power on rating score

# postiively correlated 
# short walking distance, taverna in village, shop nearby (positive) 
# morning and dinner quality, Milos island, about house (positive)*
# late (evening or night) flight, about flat (positive)
# city/town (positive), eating and drinking, host (praise), apartment (praise)

# negatively correlated
# tourist plan, market in Thira, about studio
# breakfast by host, vacation, shower issue
# ferry ride, Naxos and Paros island, bathroom condition
# airport, Caldera (crowd at the spot), staff*
# center, spacious room

# Effect of price on topic 
# probability treating price as 
# a continuous variable.
# 
# Ploting each topic separately 
# we can see that some of them increase 
# substantially with the price
max(out$meta$price)
min(out$meta$price)

for(i in 1:length(topic_labels)){
  plot(effects, covariate = "price",
       topics = i,
       model = airbnbfit, method = "continuous",
       # For this plotting we get the uper quantile
       # and low quantile of the price 
       xlab = "Price",
       xlim = c(0,1000),
       main = topic_labels[i],
       printlegend = FALSE,
       custom.labels =topic_labels[i],
       labeltype = "custom")
}

# Lets also plot them as a contrast between 
# the minimum and maximum price 
# for that we need the margins of the upper and lower quantile
margin1 <- as.numeric(quantile(out$meta$price)[2])
margin2 <- as.numeric(quantile(out$meta$price)[4])

plot(effects, covariate = "price",
     topics = c(1:47),
     model = airbnbfit, method = "difference",
     cov.value1 = margin2, cov.value2 = margin1,
     xlab = "Low Price ... High Price",
     xlim = c(-0.01,0.01),
     main = "Marginal change on topic probabilities for low and high price",
     custom.labels =topic_labels,
     labeltype = "custom")

# By looking at the two plots we can figure out
# topics which seem obvious to have a strong predictive power for the price

# negatively correlated
# highly satisfied, host (pariase), spacious room, eating and drinking*, busy and noisy spot
# Caldera (crowd at the spot), Fira, late (evening or night) flight, about house (positive)
# morning and idnner quality, private property, pool
# shower issue, vacation, breakfast by host (positive), tourist plan, boat activity

# positively correlated
# quick response (positive), apartment (positive)*, beach (positive), center
# airport, about flat (positive)*, Milos island, close restaurant, Mykonos and Rohdes island
# shop nearby (positive), transport (taxi), short walking distance
```
